{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome! In this notebook we aim to estimate the compute and memory requirements needed to train a theoretical model architecture using NoPE GPT. We'll start by first defining the parameters of the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "vocabulary_size = 50257\n",
    "embedding_dimensions = 1024\n",
    "num_q_heads = 16\n",
    "num_kv_heads = 4\n",
    "num_hidden_layers = 16\n",
    "feed_forward_ratio = 4\n",
    "\n",
    "# Training set\n",
    "tokens_per_sample = 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll estimate the total number of trainable parameters in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_embedding_params = vocabulary_size * embedding_dimensions\n",
    "\n",
    "head_dimensions = embedding_dimensions // num_q_heads\n",
    "\n",
    "kv_dimensions = head_dimensions * num_kv_heads\n",
    "\n",
    "num_attention_params = (\n",
    "    embedding_dimensions**2 + embedding_dimensions * (embedding_dimensions + 2 * kv_dimensions)\n",
    ")\n",
    "\n",
    "num_mlp_params =  embedding_dimensions * feed_forward_ratio * embedding_dimensions * 2\n",
    "\n",
    "num_rms_norm_params = embedding_dimensions\n",
    "\n",
    "parameter_counts = {\n",
    "    \"Token Embeddings\": num_embedding_params,\n",
    "    \"Attention\": num_attention_params * num_hidden_layers,\n",
    "    \"MLP\": num_mlp_params * num_hidden_layers,\n",
    "    \"RMS Norm\": num_rms_norm_params * (num_hidden_layers * 2 + 1),\n",
    "    \"Output Layer\": 0,  # Tied to token embeddings\n",
    "}\n",
    "\n",
    "plt.bar(parameter_counts.keys(), parameter_counts.values())\n",
    "\n",
    "plt.title(\"Model Parameters\")\n",
    "plt.ylabel(\"# of Parameters\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "total_parameter_count = sum(parameter_counts.values())\n",
    "\n",
    "for name, count in parameter_counts.items():\n",
    "    print(f\"{name:20s} {count:20,d} {count / total_parameter_count * 100:10.2f}%\")\n",
    "\n",
    "\n",
    "print(f\"Total parameters: {total_parameter_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of the \"shape\" of our neural network we'll look at the ratio of embedding dimensions to number of layers. Generally, an aspect ratio between 50 and 100 is considered optimal according to certain scaling laws (Kaplan, 2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_ratio = embedding_dimensions / num_hidden_layers\n",
    "\n",
    "print(f\"Network has an aspect ratio of {aspect_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same analysis for the ratio of embedding dimensions to the number of attention heads. In this case, a ratio of between 20 and 80 is considered optimal according to the same paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads_ratio = embedding_dimensions / num_q_heads\n",
    "\n",
    "print(f\"Heads ratio is {heads_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll estimate the size of the model in memory and on disk. Note that this does not include any intermediate variables that get memorized during training such as activations, gradients, optimizer state, and temporary buffers. Actual memory consumption will likely be much higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes_per_parameter = 32 // 8  # Assuming 32-bit floating point\n",
    "\n",
    "total_bytes = total_parameter_count * bytes_per_parameter\n",
    "\n",
    "total_gigabytes = total_bytes / 1e9\n",
    "\n",
    "print(f\"Total gigabytes: {total_gigabytes:,.2f}G\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can estimate the optimal number of training tokens using the Chinchilla scaling law given the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_tokens = 20 * total_parameter_count\n",
    "\n",
    "num_steps_required = round(num_training_tokens / tokens_per_sample)\n",
    "\n",
    "print(f\"Optimal training tokens: {num_training_tokens:,}\")\n",
    "\n",
    "print(f\"Steps required: {num_steps_required:,}\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll estimate the maximum number of floating point operations (FLOPs) required to perform a full forward pass of the network on a single sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops_per_matmul = 2  # Multiply + accumulate (MAC)\n",
    "ops_per_activation = 5  # Assuming SiLU\n",
    "ops_per_rms_norm = 7  # y = (x / sqrt(rms[x] + epsilon)) * gamma\n",
    "\n",
    "head_dimensions = embedding_dimensions // num_q_heads\n",
    "\n",
    "# K, Q, V projections\n",
    "attention = (\n",
    "    ops_per_matmul\n",
    "    * tokens_per_sample\n",
    "    * (embedding_dimensions * 3 * embedding_dimensions)\n",
    ")\n",
    "\n",
    "# Attention logits\n",
    "attention += (\n",
    "    ops_per_matmul * tokens_per_sample * tokens_per_sample * embedding_dimensions\n",
    ")\n",
    "\n",
    "# Reductions\n",
    "attention += (\n",
    "    ops_per_matmul\n",
    "    * num_q_heads\n",
    "    * (tokens_per_sample * tokens_per_sample * head_dimensions)\n",
    ")\n",
    "\n",
    "# Output projection\n",
    "attention += ops_per_matmul * tokens_per_sample * embedding_dimensions**2\n",
    "\n",
    "attention *= num_hidden_layers\n",
    "\n",
    "# Linear transformations\n",
    "mlp = (\n",
    "    ops_per_matmul\n",
    "    * tokens_per_sample\n",
    "    * (embedding_dimensions * (4 * embedding_dimensions))\n",
    ")\n",
    "\n",
    "mlp += (\n",
    "    ops_per_matmul\n",
    "    * tokens_per_sample\n",
    "    * ((4 * embedding_dimensions) * embedding_dimensions)\n",
    ")\n",
    "\n",
    "# Non-linear activations\n",
    "mlp += ops_per_activation * (4 * embedding_dimensions)\n",
    "\n",
    "mlp *= num_hidden_layers\n",
    "\n",
    "rms_norm = ops_per_rms_norm * embedding_dimensions * (num_hidden_layers + 1)\n",
    "\n",
    "output_layer = (\n",
    "    ops_per_matmul * tokens_per_sample * embedding_dimensions * vocabulary_size\n",
    ")\n",
    "\n",
    "flops = {\n",
    "    \"Attention\": attention,\n",
    "    \"MLP\": mlp,\n",
    "    \"RMS Norm\": rms_norm,\n",
    "    \"Output Layer\": output_layer,\n",
    "}\n",
    "\n",
    "plt.bar(flops.keys(), flops.values())\n",
    "\n",
    "plt.title(\"Model Operations\")\n",
    "plt.ylabel(\"# of FLOPs\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "total_forward_flops = sum(flops.values())\n",
    "\n",
    "for name, count in flops.items():\n",
    "    print(f\"{name:20s} {count:20,d} {count / total_forward_flops * 100:10.2f}%\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"Total forward FLOPs: {total_forward_flops:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll estimate the number of FLOPs for the backward pass. For this we use a simple heuristic of 2X the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_backward_flops = 2 * total_forward_flops\n",
    "\n",
    "print(f\"Total backward FLOPs: {total_backward_flops:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do the same for the total FLOPs per roundtrip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_roundtrip_flops = total_forward_flops + total_backward_flops\n",
    "\n",
    "print(f\"Total roundtrip FLOPs: {total_roundtrip_flops:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's estimate the number of FLOPs using the method in the PaLM paper by Chowdhery, et al. Then, we'll compare the PaLM estimation with our own as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palm_flops_per_token = (\n",
    "    6 * total_parameter_count\n",
    "    + 12 * num_hidden_layers * num_q_heads * head_dimensions * tokens_per_sample\n",
    ")\n",
    "\n",
    "total_palm_flops = palm_flops_per_token * tokens_per_sample\n",
    "\n",
    "print(f\"Total PaLM FLOPs: {total_palm_flops:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two estimates are pretty close so let's proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's estimate how long it would take to train over the optimal number of tokens given some common Nvidia Ampere generation GPU hardware configurations. Note that these results shown here are a theoretical scenario and do not factor in additional overhead such as activation checkpointing or network latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Device:\n",
    "    name: str\n",
    "    advertised_flops: float\n",
    "    mfu: float\n",
    "\n",
    "    @property\n",
    "    def actual_flops(self) -> float:\n",
    "        return self.mfu * self.advertised_flops\n",
    "\n",
    "\n",
    "devices = [\n",
    "    Device(\"RTX A2000\", 63.9e12, 0.17),\n",
    "    Device(\"RTX A4000\", 153.4e12, 0.19),\n",
    "    Device(\"RTX 3090\", 285.5e12, 0.23),\n",
    "    Device(\"A100 SXM\", 624.0e12, 0.37),\n",
    "    Device(\"HGX A100\", 4992e12, 0.30),\n",
    "]\n",
    "\n",
    "for device in devices:\n",
    "    seconds_per_step = total_roundtrip_flops / device.actual_flops\n",
    "\n",
    "    days_required = num_steps_required * seconds_per_step / 60 / 60 / 24\n",
    "\n",
    "    print(\n",
    "        f\"{device.name}: {seconds_per_step:.2f} seconds/step, {days_required:,.2f} days required\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
